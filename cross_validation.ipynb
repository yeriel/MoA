{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1u7UEnk0FyPBCqbA48xqa9jm14CBZq2yw","authorship_tag":"ABX9TyM0GZF3HGSdXj4HxeTCU2xe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#######################\n","###  Global paths   ###\n","#######################\n","custom_module_path = f'/content/drive/MyDrive/MoA/utilites'\n","dataset_path = f'/content/drive/MyDrive/MoA/dataset'"],"metadata":{"id":"v9hiuvFgEhOL","executionInfo":{"status":"ok","timestamp":1666206940225,"user_tz":180,"elapsed":11,"user":{"displayName":"leriej","userId":"06630098297104684471"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#######################\n","### Library imports ###\n","#######################\n","# standard library\n","import os\n","import sys\n","\n","# data packages\n","import numpy as np\n","import pandas as pd\n","\n","# tensorflow\n","import tensorflow as tf\n","\n","# sklearn \n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","\n","#custom tooling\n","sys.path.append(custom_module_path)\n","import preprocess"],"metadata":{"id":"Yme5U407ogv5","executionInfo":{"status":"ok","timestamp":1666206943983,"user_tz":180,"elapsed":3284,"user":{"displayName":"leriej","userId":"06630098297104684471"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["###################\n","###    Models   ###\n","###################\n","\n","def cnn(input_shape, num_classes, num_filters, size_kernel, drop):\n","    \n","    model = tf.keras.models.Sequential()\n","    \n","    # Convolucional layer\n","    model.add(tf.keras.layers.Conv1D(filters = num_filters, kernel_size = size_kernel, activation='relu', input_shape= input_shape[1:]))\n","    model.add(tf.keras.layers.MaxPooling1D())\n","    #model.add(tf.keras.layers.Conv1D(filters = num_filters, kernel_size = size_kernel, activation='relu', input_shape= input_shape[1:])   \n","    #model.add(tf.keras.layers.MaxPooling1D())\n","\n","    # Dense layer\n","    model.add(tf.keras.layers.Flatten())    \n","    model.add(tf.keras.layers.Dense(128, activation='relu'))\n","    model.add(tf.keras.layers.Dropout(drop))\n","    model.add(tf.keras.layers.Dense(64, activation='relu'))\n","    model.add(tf.keras.layers.Dropout(drop))\n","    model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))\n","\n","    # Reshape output\n","    model.add(tf.keras.layers.Reshape((num_classes,1)))\n","    \n","    return model"],"metadata":{"id":"dXIBTQGlNmDL","executionInfo":{"status":"ok","timestamp":1666206943986,"user_tz":180,"elapsed":25,"user":{"displayName":"leriej","userId":"06630098297104684471"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["###################\n","###  Utilities  ###\n","###################\n","\n","# Implementation BCEWithLogitsLoss of pytorch with keras\n","# https://stackoverflow.com/questions/59669860/implementing-bcewithlogitsloss-from-pytorch-in-keras\n","\n","def split_data(X,y, size_test=0.1):\n","    X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=size_test, random_state=19)\n","    return X_tr,X_test,y_tr,y_test \n","\n","def predict_proba(preds):\n","    preds_proba = 1 / (1 + np.exp(-preds))\n","    return preds_proba.astype(\"float32\")\n","\n","def multi_log_loss(y_pred, y_true):\n","    losses = -y_true * np.log(y_pred + 1e-15) - (1 - y_true) * np.log(1 - y_pred + 1e-15)\n","    return np.mean(losses)\n","\n","def preprocess_data(X,y):\n","    transformer = preprocess.Preprocessor() \n","    transformer.fit(X)\n","    X = transformer.transform(X)\n","    y = y.drop([\"sig_id\"], axis = 1).values.astype(\"float32\") \n","    return pd.DataFrame(X),pd.DataFrame(y)\n","\n","def reshape_data(data):\n","    nrows, nclos = data.shape\n","    return data.reshape(nrows, nclos,1)\n","\n","def get_f1_score(model, X_train, X_val, y_train, y_val):\n","    optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","    model.compile(optimizer=optimiser,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    \n","    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=30, verbose=0)\n","    \n","    y_predict = np.argmax(predict_proba(X_val), axis=1) \n","    y_val = np.argmax(y_val,axis=1)\n","    mf1 = f1_score(y_val, y_predict,average='weighted')\n","    return mf1 \n","\n","def cross_validation(X,Y,models):\n","    kf = KFold(n_splits = 10, shuffle= True)\n","\n","    for m, values in models.items():\n","        print(f'Cross Validation for model {values[0]}\\n')\n","\n","        for train_index, val_index in kf.split(X):\n","          X_train, X_val = X.iloc[train_index,], X.iloc[val_index,]\n","          y_train, y_val = Y.iloc[train_index], Y.iloc[val_index]\n","          \n","          X_train, X_val = np.array(X_train), np.array(X_val)\n","          y_train, y_val = y_train.values.astype(\"float32\"), y_val.values.astype(\"float32\")\n","          \n","          X_train, X_val = reshape_data(X_train), reshape_data(X_val)\n","          y_train, y_val = reshape_data(y_train), reshape_data(y_val)\n"," \n","          values.append(get_f1_score(m, X_train, X_val, y_train, y_val))\n","\n","        print(f'Done model {values[0]}\\n')\n","    print(f'Done')"],"metadata":{"id":"5E2ukRGrGUk5","executionInfo":{"status":"ok","timestamp":1666206943990,"user_tz":180,"elapsed":26,"user":{"displayName":"leriej","userId":"06630098297104684471"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["drugs = pd.read_csv(f'{dataset_path}/train_drug.csv')\n","train_drug = pd.read_csv(f'{dataset_path}/train_drug.csv')\n","X = pd.read_csv(f'{dataset_path}/train_features.csv')\n","y = pd.read_csv(f'{dataset_path}/train_targets_scored.csv')\n","\n","X,y = preprocess_data(X,y)\n","X, X_test, y, y_test = split_data(X,y)\n","\n","# cnn(input_shape, num_classes, num_filters, size_kernel, drop)\n","models = {cnn((21432, 877, 1), 206, 1, 3, 0.2):['cnn_0'],\n","          cnn((21432, 877, 1), 206, 1, 3, 0.4):['cnn_1'],\n","          cnn((21432, 877, 1), 206, 1, 5, 0.2):['cnn_2'],\n","          cnn((21432, 877, 1), 206, 1, 5, 0.4):['cnn_3'],\n","          cnn((21432, 877, 1), 206, 5, 3, 0.2):['cnn_4'],\n","          cnn((21432, 877, 1), 206, 5, 3, 0.4):['cnn_5'],             \n","          cnn((21432, 877, 1), 206, 5, 5, 0.2):['cnn_6'],\n","          cnn((21432, 877, 1), 206, 5, 5, 0.4):['cnn_7'],\n","          }"],"metadata":{"id":"vvvkjNbqFH2q","executionInfo":{"status":"ok","timestamp":1666206950518,"user_tz":180,"elapsed":6549,"user":{"displayName":"leriej","userId":"06630098297104684471"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["cross_validation(X,y,models)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_5yR3PsnrQ6","executionInfo":{"status":"ok","timestamp":1666212826751,"user_tz":180,"elapsed":5876252,"user":{"displayName":"leriej","userId":"06630098297104684471"}},"outputId":"90aee759-e920-49e5-cd3a-2f0e7778cff8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross Validation for model cnn_0\n","\n","Done model cnn_0\n","\n","Cross Validation for model cnn_1\n","\n","Done model cnn_1\n","\n","Cross Validation for model cnn_2\n","\n","Done model cnn_2\n","\n","Cross Validation for model cnn_3\n","\n","Done model cnn_3\n","\n","Cross Validation for model cnn_4\n","\n","Done model cnn_4\n","\n","Cross Validation for model cnn_5\n","\n","Done model cnn_5\n","\n","Cross Validation for model cnn_6\n","\n","Done model cnn_6\n","\n","Cross Validation for model cnn_7\n","\n","Done model cnn_7\n","\n","Done\n"]}]},{"cell_type":"code","source":["for m,values in models.items():\n","    print(f' model {values[0]} weighted f1-score mean is {np.mean(values[1:])}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDFJ2tiGnnTI","executionInfo":{"status":"ok","timestamp":1666212826754,"user_tz":180,"elapsed":55,"user":{"displayName":"leriej","userId":"06630098297104684471"}},"outputId":"d2ecb449-fcc3-44ec-c30d-7656f26f65d9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[" model cnn_0 weighted f1-score mean is 0.0009557834926042432\n"," model cnn_1 weighted f1-score mean is 0.0009676126484348086\n"," model cnn_2 weighted f1-score mean is 0.0008884552932962788\n"," model cnn_3 weighted f1-score mean is 0.0010302318412807898\n"," model cnn_4 weighted f1-score mean is 0.0009833984086535527\n"," model cnn_5 weighted f1-score mean is 0.0010113169825709226\n"," model cnn_6 weighted f1-score mean is 0.0009071672995420532\n"," model cnn_7 weighted f1-score mean is 0.000985270811751334\n"]}]}]}