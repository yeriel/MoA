{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "###  Global paths   ###\n",
        "#######################\n",
        "custom_module_path = f'/content/drive/MyDrive/MoA/utilites'\n",
        "dataset_path = f'/content/drive/MyDrive/MoA/dataset'"
      ],
      "metadata": {
        "id": "v9hiuvFgEhOL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "### Library imports ###\n",
        "#######################\n",
        "# standard library\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# data packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# sklearn \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#custom tooling\n",
        "sys.path.append(custom_module_path)\n",
        "import preprocess"
      ],
      "metadata": {
        "id": "Yme5U407ogv5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "###    Models   ###\n",
        "###################\n",
        "\n",
        "def cnn_0(num_classes,input_shape):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv1D(filters= 32, kernel_size=3, activation='relu',padding='same',input_shape= input_shape[1:]))\n",
        "    model.add(tf.keras.layers.MaxPooling1D())\n",
        "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3,padding='same',activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D())\n",
        "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3,padding='same',activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))\n",
        "    model.add(tf.keras.layers.Reshape((206,1)))\n",
        "    return model"
      ],
      "metadata": {
        "id": "dXIBTQGlNmDL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "###  Utilities  ###\n",
        "###################\n",
        "\n",
        "# Implementation BCEWithLogitsLoss of pytorch with keras\n",
        "# https://stackoverflow.com/questions/59669860/implementing-bcewithlogitsloss-from-pytorch-in-keras\n",
        "\n",
        "def split_data(X,y, size_test=0.1):\n",
        "    X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=size_test, random_state=19)\n",
        "    return X_tr,X_test,y_tr,y_test \n",
        "\n",
        "def predict_proba(preds):\n",
        "    preds_proba = 1 / (1 + np.exp(-preds))\n",
        "    return preds_proba.astype(\"float32\")\n",
        "\n",
        "def multi_log_loss(y_pred, y_true):\n",
        "    losses = -y_true * np.log(y_pred + 1e-15) - (1 - y_true) * np.log(1 - y_pred + 1e-15)\n",
        "    return np.mean(losses)\n",
        "\n",
        "def preprocess_data(X,y):\n",
        "    transformer = preprocess.Preprocessor() \n",
        "    transformer.fit(X)\n",
        "    X = transformer.transform(X)\n",
        "    y = y.drop([\"sig_id\"], axis = 1).values.astype(\"float32\") \n",
        "    return pd.DataFrame(X),pd.DataFrame(y)\n",
        "\n",
        "def reshape_data(data):\n",
        "    nrows, nclos = data.shape\n",
        "    return data.reshape(nrows, nclos,1)\n",
        "\n",
        "def get_f1_score(model, X_train, X_val, y_train, y_val):\n",
        "    optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=30, verbose=0)\n",
        "    \n",
        "    y_predict = np.argmax(predict_proba(X_val), axis=1) \n",
        "    y_val = np.argmax(y_val,axis=1)\n",
        "    mf1 = f1_score(y_val, y_predict,average='weighted')\n",
        "    return mf1 \n",
        "\n",
        "def cross_validation(X,Y,models):\n",
        "    kf = KFold(n_splits = 10, shuffle= True)\n",
        "\n",
        "    for m, values in models.items():\n",
        "        print(f'Cross Validation for model {values[0]}\\n')\n",
        "\n",
        "        for train_index, val_index in kf.split(X):\n",
        "          X_train, X_val = X.iloc[train_index,], X.iloc[val_index,]\n",
        "          y_train, y_val = Y.iloc[train_index], Y.iloc[val_index]\n",
        "          \n",
        "          X_train, X_val = np.array(X_train), np.array(X_val)\n",
        "          y_train, y_val = y_train.values.astype(\"float32\"), y_val.values.astype(\"float32\")\n",
        "          \n",
        "          X_train, X_val = reshape_data(X_train), reshape_data(X_val)\n",
        "          y_train, y_val = reshape_data(y_train), reshape_data(y_val)\n",
        " \n",
        "          values.append(get_f1_score(m, X_train, X_val, y_train, y_val))\n",
        "\n",
        "        print(f'Done model {values[0]}\\n')\n",
        "    print(f'Done')"
      ],
      "metadata": {
        "id": "5E2ukRGrGUk5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drugs = pd.read_csv(f'{dataset_path}/train_drug.csv')\n",
        "train_drug = pd.read_csv(f'{dataset_path}/train_drug.csv')\n",
        "X = pd.read_csv(f'{dataset_path}/train_features.csv')\n",
        "y = pd.read_csv(f'{dataset_path}/train_targets_scored.csv')\n",
        "\n",
        "X,y = preprocess_data(X,y)\n",
        "X, X_test, y, y_test = split_data(X,y)\n",
        "\n",
        "models = {cnn_0(206,(21432, 877, 1)):['cnn_0']}\n",
        "cross_validation(X,y,models)\n",
        "\n",
        "for m,values in models.items():\n",
        "    print(f' model {values[0]} weighted f1-score mean is {np.mean(values[1:])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvvkjNbqFH2q",
        "outputId": "c53b6bbb-a409-4f89-e671-331cb1317ebf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation for model cnn_0\n",
            "\n",
            "Done model cnn_0\n",
            "\n",
            "Done\n",
            " model cnn_0 weighted f1-score mean is 0.0009872659942659616\n"
          ]
        }
      ]
    }
  ]
}